{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WVZhjuh-QXe",
        "outputId": "ecf08375-2990-44e1-b4fd-89b3be58ac8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=94ef3482deffdeab8ca879023332c7f346e9b6819d52c657483ec4cb399c79c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: portalocker, colorama, sacrebleu, rouge-score\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 rouge-score-0.1.2 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_names_dir = \"/content/drive/MyDrive/cpsc532/male_names.csv\"\n",
        "female_names_dir = \"/content/drive/MyDrive/cpsc532/female_names.csv\"\n",
        "unisex_names_dir = \"/content/drive/MyDrive/cpsc532/unisex_names.csv\"\n",
        "events_dir = \"/content/drive/MyDrive/cpsc532/events_extracted_processed.txt\"\n",
        "\n",
        "model_dir = \"/content/drive/MyDrive/cpsc532/comet-atomic_2020_BART\"\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/cpsc532/output\""
      ],
      "metadata": {
        "id": "Ct2ztWeFaySL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script from COMET-ATOMIC"
      ],
      "metadata": {
        "id": "o08wPYr0OxLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code from https://github.com/allenai/comet-atomic-2020/blob/master/models/comet_atomic2020_bart/generation_example.py\n",
        "\n",
        "\n",
        "**You need to put the utils.py (https://github.com/allenai/comet-atomic-2020/blob/master/models/comet_atomic2020_bart/utils.py) in the same directory**"
      ],
      "metadata": {
        "id": "FX-afEwmbrAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code from https://github.com/allenai/comet-atomic-2020/blob/master/models/comet_atomic2020_bart/generation_example.py\n",
        "# put the utils.py (https://github.com/allenai/comet-atomic-2020/blob/master/models/comet_atomic2020_bart/utils.py) in the same directory\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from utils import calculate_rouge, use_task_specific_params, calculate_bleu_score, trim_batch\n",
        "\n",
        "\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "\n",
        "class Comet:\n",
        "    def __init__(self, model_path):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        task = \"summarization\"\n",
        "        use_task_specific_params(self.model, task)\n",
        "        self.batch_size = 1\n",
        "        self.decoder_start_token_id = None\n",
        "\n",
        "    def generate(\n",
        "            self,\n",
        "            queries,\n",
        "            decode_method=\"beam\",\n",
        "            num_generate=5,\n",
        "            ):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            examples = queries\n",
        "\n",
        "            decs = []\n",
        "            for batch in list(chunks(examples, self.batch_size)):\n",
        "\n",
        "                batch = self.tokenizer(batch, return_tensors=\"pt\", truncation=True, padding=\"max_length\").to(self.device)\n",
        "                input_ids, attention_mask = trim_batch(**batch, pad_token_id=self.tokenizer.pad_token_id)\n",
        "\n",
        "                summaries = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    decoder_start_token_id=self.decoder_start_token_id,\n",
        "                    num_beams=num_generate,\n",
        "                    num_return_sequences=num_generate,\n",
        "                    )\n",
        "\n",
        "                dec = self.tokenizer.batch_decode(summaries, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "                decs.append(dec)\n",
        "\n",
        "            return decs\n",
        "\n",
        "\n",
        "all_relations = [\n",
        "    \"AtLocation\",\n",
        "    \"CapableOf\",\n",
        "    \"Causes\",\n",
        "    \"CausesDesire\",\n",
        "    \"CreatedBy\",\n",
        "    \"DefinedAs\",\n",
        "    \"DesireOf\",\n",
        "    \"Desires\",\n",
        "    \"HasA\",\n",
        "    \"HasFirstSubevent\",\n",
        "    \"HasLastSubevent\",\n",
        "    \"HasPainCharacter\",\n",
        "    \"HasPainIntensity\",\n",
        "    \"HasPrerequisite\",\n",
        "    \"HasProperty\",\n",
        "    \"HasSubEvent\",\n",
        "    \"HasSubevent\",\n",
        "    \"HinderedBy\",\n",
        "    \"InheritsFrom\",\n",
        "    \"InstanceOf\",\n",
        "    \"IsA\",\n",
        "    \"LocatedNear\",\n",
        "    \"LocationOfAction\",\n",
        "    \"MadeOf\",\n",
        "    \"MadeUpOf\",\n",
        "    \"MotivatedByGoal\",\n",
        "    \"NotCapableOf\",\n",
        "    \"NotDesires\",\n",
        "    \"NotHasA\",\n",
        "    \"NotHasProperty\",\n",
        "    \"NotIsA\",\n",
        "    \"NotMadeOf\",\n",
        "    \"ObjectUse\",\n",
        "    \"PartOf\",\n",
        "    \"ReceivesAction\",\n",
        "    \"RelatedTo\",\n",
        "    \"SymbolOf\",\n",
        "    \"UsedFor\",\n",
        "    \"isAfter\",\n",
        "    \"isBefore\",\n",
        "    \"isFilledBy\",\n",
        "    \"oEffect\",\n",
        "    \"oReact\",\n",
        "    \"oWant\",\n",
        "    \"xAttr\",\n",
        "    \"xEffect\",\n",
        "    \"xIntent\",\n",
        "    \"xNeed\",\n",
        "    \"xReact\",\n",
        "    \"xReason\",\n",
        "    \"xWant\",\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "OXkNakcw-N3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Inferences"
      ],
      "metadata": {
        "id": "1IExfSOQO7oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "FnYVKzbKL-YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtgJLIUHEC_B",
        "outputId": "180921ac-b18e-4d11-8980-2eccd6a7e380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Control the randomness"
      ],
      "metadata": {
        "id": "raia5JEVP7JG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Function to control randomness in the code.\"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "yJe7kC0WI6UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the model (GPT or BART)"
      ],
      "metadata": {
        "id": "LKecd8giP1gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"model loading ...\")\n",
        "comet = Comet(model_dir)\n",
        "comet.model.zero_grad()\n",
        "print(\"model loaded\")"
      ],
      "metadata": {
        "id": "bANPI5zcNOP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc03191a-4e2b-40c5-c431-cb17d2393fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model loading ...\n",
            "model loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load files"
      ],
      "metadata": {
        "id": "fCgFQidSP-M-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_names(file_path):\n",
        "    \"\"\"Load names from a CSV file assuming each name is in a single column.\"\"\"\n",
        "    return pd.read_csv(file_path, header=None)[0].tolist()[1:]\n",
        "\n",
        "def load_events(file_path):\n",
        "    \"\"\"Load events from a text file and remove numbering.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        events = [line.split(\". \", 1)[1].strip() for line in file if \". \" in line]\n",
        "    return events\n",
        "\n",
        "def assign_names_to_events(names, events):\n",
        "  assigned_events = []\n",
        "  for event in events:\n",
        "    name = random.choice(names)\n",
        "    assigned_event = f\"{name} {event}\"\n",
        "    assigned_events.append(assigned_event)\n",
        "  return assigned_events\n",
        "\n"
      ],
      "metadata": {
        "id": "RglA550JHkoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "male_names = load_names(male_names_dir)\n",
        "female_names = load_names(female_names_dir)\n",
        "unisex_names = load_names(unisex_names_dir)\n",
        "events = load_events(events_dir)\n"
      ],
      "metadata": {
        "id": "q618NPhKH2Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get and store the inferences"
      ],
      "metadata": {
        "id": "ebFpg3GUQEpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_inferences(events, relations, model):\n",
        "  results = []\n",
        "\n",
        "  for head in tqdm(events, desc = \"Events\"):\n",
        "    for rel in relations:\n",
        "      query = \"{} {} [GEN]\".format(head, rel)\n",
        "      output = model.generate([query], decode_method=\"beam\", num_generate=5)\n",
        "      results.append({\n",
        "          \"Event\": head,\n",
        "          \"Relation\": rel,\n",
        "          \"Query\": query,\n",
        "          \"Inference\": output\n",
        "      })\n",
        "  return results"
      ],
      "metadata": {
        "id": "9_wVt7XDOPKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\"PersonX\"]\n",
        "assigned_events = assign_names_to_events(names, events)\n",
        "results = get_inferences(assigned_events, all_relations, comet)\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "df = pd.DataFrame(results)\n",
        "output_file = f\"{output_dir}/comet_inferences_PersonX_{timestamp}.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Batch inferences saved to '{output_file}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNZObeos_JfK",
        "outputId": "b4b71d3e-ff79-4612-c50b-b1b21c5329a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Events: 100%|██████████| 400/400 [35:46<00:00,  5.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch inferences saved to '/content/drive/MyDrive/cpsc532/output/comet_inferences_PersonX_2025-03-28_18-59-02.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genders = [\"female\", \"male\", \"unisex\"]\n",
        "for gender in genders:\n",
        "  if gender == \"female\":\n",
        "    names = female_names\n",
        "  elif gender == \"male\":\n",
        "    names = male_names\n",
        "  else:\n",
        "    names = unisex_names\n",
        "  assigned_events = assign_names_to_events(names, events)\n",
        "  results = get_inferences(assigned_events, all_relations, comet)\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  df = pd.DataFrame(results)\n",
        "  output_file = f\"{output_dir}/comet_inferences_{gender}_{timestamp}.csv\"\n",
        "  df.to_csv(output_file, index=False)\n",
        "  print(f\"Batch inferences saved to '{output_file}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9thk74xOTpTY",
        "outputId": "6bc954de-fcc3-4f01-8d6c-eaac34139a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvents:   0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1532: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Events: 100%|██████████| 400/400 [33:42<00:00,  5.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch inferences saved to '/content/drive/MyDrive/cpsc532/output/comet_inferences_female_2025-03-25_00-06-56.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Events: 100%|██████████| 400/400 [33:33<00:00,  5.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch inferences saved to '/content/drive/MyDrive/cpsc532/output/comet_inferences_male_2025-03-25_00-40-30.csv'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Events: 100%|██████████| 400/400 [33:07<00:00,  4.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch inferences saved to '/content/drive/MyDrive/cpsc532/output/comet_inferences_unisex_2025-03-25_01-13-38.csv'\n"
          ]
        }
      ]
    }
  ]
}